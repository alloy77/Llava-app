# ğŸ™ï¸ Multimodal RAG App â€“ Image + Audio Interaction using OpenAI Whisper & LLaVA

A voice-interactive multimodal assistant that describes images and answers questions about them using your **voice**. It combines the power of **OpenAI Whisper** for speech recognition and **LLaVA-1.5 (Language + Vision)** for detailed visual understanding. Built with ğŸ¤— Transformers, ğŸ§  Whisper, ğŸ¨ PIL, ğŸ”Š gTTS, and deployed via Gradio.

---

## ğŸš€ Features

- ğŸ¤ **Speech-to-Text** with OpenAI Whisper (Multilingual)
- ğŸ–¼ï¸ **Image Understanding** via LLaVA-1.5 7B (4-bit quantized for efficiency)
- ğŸ¤– **Prompt-Driven Visual Reasoning**: Ask questions like *"What color is the microphone?"*
- ğŸ”Š **Text-to-Speech Response** using Google TTS
- ğŸŒ **Gradio Web UI** for interactive multimodal querying

---

## ğŸ“¦ Tech Stack

| Module            | Description                          |
|------------------|--------------------------------------|
| `transformers`    | LLaVA-1.5 image-to-text pipeline     |
| `bitsandbytes`    | 4-bit quantization for LLaVA         |
| `whisper`         | Speech-to-text (OpenAI Whisper)      |
| `gTTS`            | Text-to-speech conversion            |
| `Gradio`          | Web UI to interact with app          |
| `NLTK`            | Sentence tokenization for parsing    |
| `FFmpeg`          | Audio preprocessing support          |

---

## ğŸ§  How It Works

1. **User speaks** a query using a microphone.
2. **Whisper** converts the audio into text.
3. **LLaVA** receives the image and the query to generate a visual response.
4. The assistant's **reply is spoken back** using Google Text-to-Speech.
5. Everything is logged and timestamped for traceability.

---

## ğŸ“¸ Sample Output

**Input Image**:  
![Sample Image](img.jpg)

**Query**:  
`"What color is the microphone in the image?"`

**LLaVA Output**:
> *"The microphone in the image is black."*

**Audio Response**:  
> ğŸ”ˆ Plays generated response via gTTS.

---

## ğŸ› ï¸ Setup Instructions

### 1. Install Dependencies

```bash
pip install -q -U transformers==4.37.2
pip install -q bitsandbytes==0.41.3 accelerate==0.25.0
pip install -q git+https://github.com/openai/whisper.git
pip install -q gradio gTTS
```

### 2. Clone and Run

```bash
git clone https://github.com/alloy77/Llava-app.git
cd Llava-app
python app.py  # or open in Jupyter/Colab
```

---

## ğŸ§ª Run in Colab (Recommended)

- Automatically sets up everything
- Launches the Gradio app with microphone + image upload

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alloy77/Llava-app/blob/main/Multimodal_RAG.ipynb)

---

## ğŸ§¬ Components

- `img2txt()`: Combines image + prompt and extracts LLaVA output
- `transcribe()`: Converts voice input to text via Whisper
- `text_to_speech()`: Converts AI response to audio using gTTS
- `gr.Interface`: Gradio UI to glue everything together

---

## ğŸ“ File Structure

```
Llava-app/
â”œâ”€â”€ Multimodal_RAG.ipynb       # Main notebook
â”œâ”€â”€ img.jpg                    # Sample image input
â”œâ”€â”€ Temp.mp3 / Temp3.mp3       # Temporary audio files
â”œâ”€â”€ *.txt                      # Log files
â””â”€â”€ README.md                  # Project documentation
```

---

## âœ… TODO / Future Enhancements

- ğŸ” Hugging Face Token integration for authenticated models
- ğŸ¯ Improve prompt engineering for better reasoning
- ğŸ—£ï¸ Add language selection for Whisper + gTTS
- ğŸ’¾ Save audio conversations for reuse
- ğŸ§  Connect to RAG for knowledge-grounded answers

---

## ğŸ¤ Contributing

Open to suggestions, ideas, or collaborations! Feel free to fork and submit a PR.

